{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3KlvI8rY7BU"
      },
      "source": [
        "# **Image Classification Using SVM with Various Feature Extraction**\n",
        "\n",
        "This project classifies images as \"*Glazed*\" or \"*Unglazed*\" using SVM models trained on three different feature extraction methods:\n",
        "\n",
        "1. **Color Histogram**\n",
        "2. **HOG (Histogram of Oriented Gradients)**\n",
        "3. **SIFT + ORB Combined Descriptors**\n",
        "\n",
        "## **Dataset Summary**\n",
        "- **Train Set (75%)**: 18 images (9 for each class)\n",
        "- **Test Set (25%)**: 6 images (3 for each class)\n",
        "\n",
        "## **Key Results**\n",
        "The models were evaluated based on precision, recall, F1-score, and accuracy to identify the most effective feature extraction methods for distinguishing images between the classes.\n",
        "\n",
        "This evaluation focuses on determining which feature extraction technique provides the most consistent and reliable performance for the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN2Xvm23Y9i3",
        "outputId": "e9bbbb3b-681b-4ec5-c459-8c938c7fa144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Color Histogram Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Unglazed       0.50      0.33      0.40         3\n",
            "      Glazed       0.50      0.67      0.57         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n",
            "Classification Report for HOG Features Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Unglazed       0.00      0.00      0.00         3\n",
            "      Glazed       0.50      1.00      0.67         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.25      0.50      0.33         6\n",
            "weighted avg       0.25      0.50      0.33         6\n",
            "\n",
            "Classification Report for SIFT+ORB Features Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Unglazed       0.50      0.33      0.40         3\n",
            "      Glazed       0.50      0.67      0.57         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from skimage.feature import hog\n",
        "from PIL import Image\n",
        "\n",
        "# Load and Preprocess Images\n",
        "def load_images(dataset_path, image_size=(256, 256)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(dataset_path)\n",
        "\n",
        "    for class_index, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img = img.resize(image_size)\n",
        "            images.append(np.array(img))\n",
        "            labels.append(class_index)\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# Feature Extraction Functions\n",
        "def extract_color_histogram(image):\n",
        "    chans = cv2.split(image)\n",
        "    hist = [cv2.calcHist([chan], [0], None, [256], [0, 256]).flatten() for chan in chans]\n",
        "    return np.concatenate(hist)\n",
        "\n",
        "def extract_hog_features(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    features, _ = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "    return features\n",
        "\n",
        "def extract_sift_orb_features(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # SIFT\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints_sift, descriptors_sift = sift.detectAndCompute(gray, None)\n",
        "\n",
        "    # ORB\n",
        "    orb = cv2.ORB_create()\n",
        "    keypoints_orb, descriptors_orb = orb.detectAndCompute(gray, None)\n",
        "\n",
        "    max_descriptors = 512\n",
        "\n",
        "    if descriptors_sift is not None:\n",
        "        descriptors_sift = descriptors_sift[:max_descriptors]  # Limit to max descriptors\n",
        "    else:\n",
        "        descriptors_sift = np.zeros((max_descriptors, 128))  # Padding if no descriptors\n",
        "\n",
        "    if descriptors_orb is not None:\n",
        "        descriptors_orb = descriptors_orb[:max_descriptors]  # Limit to max descriptors\n",
        "    else:\n",
        "        descriptors_orb = np.zeros((max_descriptors, 32))  # Padding if no descriptors\n",
        "\n",
        "    # Ensure fixed-length descriptors (padding to max descriptors)\n",
        "    descriptors_sift = np.pad(descriptors_sift, ((0, max_descriptors - descriptors_sift.shape[0]), (0, 0)), mode='constant')\n",
        "    descriptors_orb = np.pad(descriptors_orb, ((0, max_descriptors - descriptors_orb.shape[0]), (0, 0)), mode='constant')\n",
        "\n",
        "    # Flatten descriptors to create a fixed-size feature vector\n",
        "    combined_descriptors = np.hstack((descriptors_sift.flatten(), descriptors_orb.flatten()))\n",
        "\n",
        "    return combined_descriptors\n",
        "\n",
        "# Helper to extract features\n",
        "def extract_features(images, feature_extractor):\n",
        "    features = []\n",
        "    for image in images:\n",
        "        feature = feature_extractor(image)\n",
        "        features.append(feature)\n",
        "    return np.array(features)\n",
        "\n",
        "# Load train and test images\n",
        "train_path = \"/content/drive/MyDrive/Coding/CV GLAZE/Dataset New/Train\"\n",
        "test_path = \"/content/drive/MyDrive/Coding/CV GLAZE/Dataset New/Test\"\n",
        "image_size = (256, 256)\n",
        "\n",
        "# Load train and test images\n",
        "train_images, train_labels, class_names_train = load_images(train_path, image_size)\n",
        "test_images, test_labels, class_names_test = load_images(test_path, image_size)\n",
        "\n",
        "# Extract features for each method (train set)\n",
        "X_train_color = extract_features(train_images, extract_color_histogram)\n",
        "X_train_hog = extract_features(train_images, extract_hog_features)\n",
        "X_train_sift_orb = extract_features(train_images, extract_sift_orb_features)\n",
        "\n",
        "# Standardize Features (train set)\n",
        "scaler_color = StandardScaler()\n",
        "X_train_color = scaler_color.fit_transform(X_train_color)\n",
        "\n",
        "scaler_hog = StandardScaler()\n",
        "X_train_hog = scaler_hog.fit_transform(X_train_hog)\n",
        "\n",
        "scaler_sift_orb = StandardScaler()\n",
        "X_train_sift_orb = scaler_sift_orb.fit_transform(X_train_sift_orb)\n",
        "\n",
        "# Train SVM Models function\n",
        "def train_svm(X_train, y_train, kernel='linear'):\n",
        "    svm_model = SVC(kernel=kernel, random_state=42)\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    return svm_model\n",
        "\n",
        "# Train models with different feature sets\n",
        "svm_model_color = train_svm(X_train_color, train_labels)\n",
        "svm_model_hog = train_svm(X_train_hog, train_labels)\n",
        "svm_model_sift_orb = train_svm(X_train_sift_orb, train_labels)\n",
        "\n",
        "# Function to preprocess the image and extract features (test images)\n",
        "def preprocess_image(img, image_size=(256, 256)):\n",
        "    # Resize and convert the image to RGB\n",
        "    img = Image.fromarray(img).convert('RGB')\n",
        "    img = img.resize(image_size)\n",
        "    img = np.array(img)\n",
        "\n",
        "    # Extract features for each method\n",
        "    color_features = extract_color_histogram(img)\n",
        "    hog_features = extract_hog_features(img)\n",
        "    sift_orb_features = extract_sift_orb_features(img)\n",
        "\n",
        "    # Standardize the features using the trained scalers\n",
        "    color_features = scaler_color.transform([color_features])\n",
        "    hog_features = scaler_hog.transform([hog_features])\n",
        "    sift_orb_features = scaler_sift_orb.transform([sift_orb_features])\n",
        "\n",
        "    return color_features, hog_features, sift_orb_features\n",
        "\n",
        "# Predict using all three models (test images)\n",
        "def predict_image_class(img):\n",
        "    # Preprocess the input image\n",
        "    color_features, hog_features, sift_orb_features = preprocess_image(img)\n",
        "\n",
        "    # Make predictions using the three models\n",
        "    prediction_color = svm_model_color.predict(color_features)\n",
        "    prediction_hog = svm_model_hog.predict(hog_features)\n",
        "    prediction_sift_orb = svm_model_sift_orb.predict(sift_orb_features)\n",
        "\n",
        "    # Return the predictions from all three models\n",
        "    return prediction_color[0], prediction_hog[0], prediction_sift_orb[0]\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_color = []\n",
        "y_pred_hog = []\n",
        "y_pred_sift_orb = []\n",
        "\n",
        "for img in test_images:\n",
        "    prediction_color, prediction_hog, prediction_sift_orb = predict_image_class(img)\n",
        "    y_pred_color.append(prediction_color)\n",
        "    y_pred_hog.append(prediction_hog)\n",
        "    y_pred_sift_orb.append(prediction_sift_orb)\n",
        "\n",
        "# Convert predictions to numpy arrays\n",
        "y_pred_color = np.array(y_pred_color)\n",
        "y_pred_hog = np.array(y_pred_hog)\n",
        "y_pred_sift_orb = np.array(y_pred_sift_orb)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report for Color Histogram Model:\")\n",
        "print(classification_report(test_labels, y_pred_color, target_names=class_names_test))\n",
        "\n",
        "print(\"Classification Report for HOG Features Model:\")\n",
        "print(classification_report(test_labels, y_pred_hog, target_names=class_names_test))\n",
        "\n",
        "print(\"Classification Report for SIFT+ORB Features Model:\")\n",
        "print(classification_report(test_labels, y_pred_sift_orb, target_names=class_names_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Save Model**"
      ],
      "metadata": {
        "id": "L826W3Wqw2tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the SVM models\n",
        "with open('svm_model_color.pkl', 'wb') as file:\n",
        "    pickle.dump(svm_model_color, file)\n",
        "\n",
        "with open('svm_model_hog.pkl', 'wb') as file:\n",
        "    pickle.dump(svm_model_hog, file)\n",
        "\n",
        "with open('svm_model_sift_orb.pkl', 'wb') as file:\n",
        "    pickle.dump(svm_model_sift_orb, file)\n",
        "\n",
        "# Save the scalers\n",
        "with open('scaler_color.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler_color, file)\n",
        "\n",
        "with open('scaler_hog.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler_hog, file)\n",
        "\n",
        "with open('scaler_sift_orb.pkl', 'wb') as file:\n",
        "    pickle.dump(scaler_sift_orb, file)\n",
        "\n",
        "print(\"Models and scalers have been saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iUEBP--w4Ta",
        "outputId": "a51f4b10-7737-482b-83ba-59c591dfb1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models and scalers have been saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXn5ZkNnbwiO"
      },
      "source": [
        "## **Evaluation and Conclusion for the Classification**\n",
        "\n",
        "The three models show distinct performances based on their evaluation metrics:\n",
        "\n",
        "- **Color Histogram Model**:  \n",
        "  Achieved an accuracy of **50%**, with a balanced precision of **0.5** for both classes. The recall for the *glazed* class is higher (**0.67**), leading to a slightly better F1-score of **0.57** for this class.\n",
        "\n",
        "- **HOG Features Model**:  \n",
        "  Also reached **50%** accuracy but struggled with the *unglazed* class, having a precision, recall, and F1-score of **0.0**. Its performance relies entirely on correctly identifying the *glazed* class (F1-score: **0.67**).\n",
        "\n",
        "- **SIFT + ORB Features Model**:  \n",
        "  Performed identically to the Color Histogram Model with an accuracy of **50%** and similar class-wise F1-scores, indicating balanced predictions.\n",
        "\n",
        "### Conclusion:  \n",
        "The **Color Histogram** and **SIFT + ORB Features** models demonstrate more balanced predictions across classes compared to the **HOG Features Model**, which is highly biased. Future improvements should focus on increasing dataset size and refining feature extraction methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZZbq9HTiyc8"
      },
      "source": [
        "# **Keypoint Matching Evaluation**\n",
        "\n",
        "To measure the effectiveness of the glazing technique, we used **SIFT and ORB feature extraction methods alongside the BFMatcher algorithm**. The goal is to identify how well the glazing disrupts local feature matching between glazed and unglazed images. A **lower match percentage indicates higher disruption, demonstrating the effectiveness** of the glazing technique.\n",
        "\n",
        "Below is the code used for keypoint matching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UM7KG3PWi2gI",
        "outputId": "d93bc09e-4b47-4c4c-9964-019df5ee4e65"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set directories\n",
        "glazed_dir = '/content/drive/MyDrive/Coding/CV GLAZE/Keypoint Matching/Glazed'\n",
        "unglazed_dir = '/content/drive/MyDrive/Coding/CV GLAZE/Keypoint Matching/Unglazed'\n",
        "\n",
        "# Initialize SIFT and ORB\n",
        "sift = cv2.SIFT_create()\n",
        "orb = cv2.ORB_create()\n",
        "\n",
        "# Function for keypoint matching using BFMatcher\n",
        "def keypoint_matching(img1, img2, feature_extractor):\n",
        "    kp1, des1 = feature_extractor.detectAndCompute(img1, None)\n",
        "    kp2, des2 = feature_extractor.detectAndCompute(img2, None)\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    return kp1, kp2, matches\n",
        "\n",
        "# Get the number of images in the directories\n",
        "glazed_files = [f for f in os.listdir(glazed_dir) if f.endswith('.jpg')]\n",
        "unglazed_files = [f for f in os.listdir(unglazed_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# Ensure both directories have the same number of images\n",
        "num_images = min(len(glazed_files), len(unglazed_files))\n",
        "\n",
        "# Loop through the available image files\n",
        "for i in range(num_images):\n",
        "    # Read images\n",
        "    img_glazed = cv2.imread(os.path.join(glazed_dir, glazed_files[i]))\n",
        "    img_unglazed = cv2.imread(os.path.join(unglazed_dir, unglazed_files[i]))\n",
        "\n",
        "    # Resize images for visualization\n",
        "    img_glazed, img_unglazed = cv2.resize(img_glazed, (600, 600)), cv2.resize(img_unglazed, (600, 600))\n",
        "\n",
        "    # Perform keypoint matching with SIFT and ORB\n",
        "    kp_glazed_sift, kp_unglazed_sift, matches_sift = keypoint_matching(img_glazed, img_unglazed, sift)\n",
        "    kp_glazed_orb, kp_unglazed_orb, matches_orb = keypoint_matching(img_glazed, img_unglazed, orb)\n",
        "\n",
        "    # Calculate match percentages for SIFT and ORB\n",
        "    match_percentage_sift = (len(matches_sift) / min(len(kp_glazed_sift), len(kp_unglazed_sift))) * 100 if len(kp_glazed_sift) > 0 and len(kp_unglazed_sift) > 0 else 0\n",
        "    match_percentage_orb = (len(matches_orb) / min(len(kp_glazed_orb), len(kp_unglazed_orb))) * 100 if len(kp_glazed_orb) > 0 and len(kp_unglazed_orb) > 0 else 0\n",
        "\n",
        "    # Combine SIFT and ORB matches, remove duplicates\n",
        "    combined_matches = list({match.queryIdx: match for match in matches_sift + matches_orb}.values())\n",
        "\n",
        "    # Draw matches\n",
        "    img_matches_combined = cv2.drawMatches(img_glazed, kp_glazed_sift, img_unglazed, kp_unglazed_sift, combined_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(cv2.cvtColor(img_matches_combined, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f'Image {i} - Combined Matches: {len(combined_matches)} (SIFT: {match_percentage_sift:.2f}% | ORB: {match_percentage_orb:.2f}%)')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion from BF Matcher Results**\n",
        "\n",
        "The evaluation was conducted on sample images, where the average match percentage for both SIFT and ORB ranged between **60% and 70%**. These results indicate that the glazing technique **successfully modifies** the visual style of the glazed images. While some matches remain, the **60-70% range** demonstrates that the technique effectively reduces local feature matching, making it harder for AI models to replicate the original style."
      ],
      "metadata": {
        "id": "53BWdl9L1JtO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}